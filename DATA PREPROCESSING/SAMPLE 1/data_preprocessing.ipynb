{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8252e516",
   "metadata": {},
   "source": [
    "# üßπ Data Pre-Processing\n",
    "\n",
    "### üìì Definition\n",
    "\n",
    "Data preprocessing refers to the cleaning, transforming, and integrating of data in order to make it ready for analysis.\n",
    "\n",
    "### üîπ What we will do in this section?\n",
    "\n",
    "- Importing the basic libraries.\n",
    "- Importing the dataset directly from **<a href=\"https://github.com/rikisupriyo/end-to-end-ml/tree/main/DATASETS\">here</a>**.\n",
    "- Checking if the dataset got any missing values or not.\n",
    "- Replacing the missing values (if any).\n",
    "- Seperating the dependent and independent variables from the dataset.\n",
    "- OneHotEncoding the categorical variables column from the dataset.\n",
    "- Splitting the dataset into training and testing set.\n",
    "- Feature Scaling the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2933a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing basic libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a0540c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "\n",
    "dataset = pd.read_csv('https://raw.githubusercontent.com/rikisupriyo/end-to-end-ml/main/DATASETS/OTHERS/preprocessing.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c1d0026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Country    10 non-null     object \n",
      " 1   Age        9 non-null      float64\n",
      " 2   Salary     9 non-null      float64\n",
      " 3   Purchased  10 non-null     object \n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 452.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Checking the datatype of each columns \n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cf32d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country\n",
       "France     4\n",
       "Spain      3\n",
       "Germany    3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the number of states in our dataset\n",
    "\n",
    "dataset['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e85fb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country      0\n",
       "Age          1\n",
       "Salary       1\n",
       "Purchased    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there are any missing values in the dataset we need to replace\n",
    "\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0276f7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (10, 3)\n",
      "Shape of y: (10,)\n"
     ]
    }
   ],
   "source": [
    "# Seperating the dependent and independent variables into X and y respectively\n",
    "\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "# Checking the shape of X and y\n",
    "\n",
    "print(f'Shape of X: {X.shape}\\nShape of y: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14878294",
   "metadata": {},
   "source": [
    "### üìù NOTE:\n",
    "\n",
    "As we can see the **Age** and **Salary** column got missing values in it. So we can replace the missing values with the mean of the values in each of those two columns respectively. We can use SimpleImputer from sklearn to do this operation easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3609419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X[:, 1:] = imputer.fit_transform(X[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16315ca1",
   "metadata": {},
   "source": [
    "### üìù NOTE:\n",
    "\n",
    "Now we need to OneHotEncode the **Country** column and LabelEncode the **Puchased** column to transform the text data to numerical data for our machine to understand the data properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "864e0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehotencoding the column containing the Countries\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "transformer = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "X = np.array(transformer.fit_transform(X))\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c20977e",
   "metadata": {},
   "source": [
    "### üîë IMPORTANT NOTE:\n",
    "\n",
    "When we use OneHotEncode on a specific column, the encoded column comes get split into the same amount of columns as the value count of that column and as you can see, it also appears infront of the dataset. In our case the **Country** column got seperated into 3 columns because our value count for this column is 3. Here the most important thing we need to do is omit one of the columns we got from OneHotEncoding to avoid the **Dummy Variable** trap and not ending up with correlated features. It is a very good practice to do so. If we got ***k*** columns from the encoding, we must use only ***k-1*** columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9a2226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : [[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]]\n",
      "y : [0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Printing the first 5 rows from X and y\n",
    "\n",
    "print(f'X : {X[:5]}')\n",
    "print(f'y : {y[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99747ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 44.0 72000.0]\n",
      " [0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 30.0 54000.0]\n",
      " [0.0 1.0 38.0 61000.0]\n",
      " [1.0 0.0 40.0 63777.77777777778]]\n"
     ]
    }
   ],
   "source": [
    "# dropping the first column to avoid dummy variable trap\n",
    "\n",
    "X = X[:, 1:]\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b60db63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (8, 4)\n",
      "Shape of y_train: (8,)\n",
      "Shape of X_test: (2, 4)\n",
      "Shape of y_test: (2,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into train and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(f'Shape of X_train: {X_train.shape}\\nShape of y_train: {y_train.shape}\\nShape of X_test: {X_test.shape}\\nShape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a562952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 -1.4588292694047795 -0.9016629672292141]]\n"
     ]
    }
   ],
   "source": [
    "# Standardscaling the data to avoid large gaps between each numbers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[:, 2:] = scaler.fit_transform(X_train[:, 2:])\n",
    "X_test[:, 2:] = scaler.transform(X_test[:, 2:])\n",
    "\n",
    "# Printing the first 5 rows of the Standardized dataset\n",
    "\n",
    "print(X_test[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e69825",
   "metadata": {},
   "source": [
    "### üîë IMPORTANT NOTE:\n",
    "\n",
    "We should always feature scale our data only after splitting our data into training and testing set because if we apply it before the split then it will actually get the mean and the standard deviation of all the values, including the ones in the test set. Since the test set is something we are supposed to have like some future data in production, applying feature scaling on the original data set, before the split would cause some information leakage on the test set. That's why it's a very good practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e47b6d",
   "metadata": {},
   "source": [
    "# ‚úÖ DONE!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
